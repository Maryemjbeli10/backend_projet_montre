"""
Preprocessing et Feature Engineering - Fichier 03
Entr√©e: watches.csv
Sortie: watches_cleaned.csv (pr√™t pour le modeling)
Avec validation crois√©e des imputations
"""
import os
import sys
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from pathlib import Path
from sklearn.model_selection import KFold

from .setup import RAW_CSV, CLEANED_CSV, PROCESSED_DATA_DIR

CURRENT_YEAR = 2026
RANDOM_STATE = 42

def detect_separator(path):
    with open(path, 'r', encoding='utf-8') as f:
        first_line = f.readline()
        return ';' if ';' in first_line else ','

def load_raw():
    """Charge les donn√©es brutes"""
    sep = detect_separator(RAW_CSV)
    df = pd.read_csv(RAW_CSV, sep=sep)
    df.columns = [c.strip().replace("\u00a0", " ").strip() for c in df.columns]
    return df

# ============================================================================
# √âTAPE 1: NETTOYAGE
# ============================================================================

def remove_duplicates(df):
    """Supprime les lignes dupliqu√©es"""
    before = len(df)
    df = df.drop_duplicates()
    print(f"‚úÖ Duplications: {before - len(df)} lignes supprim√©es")
    return df

def clean_price(df):
    """Nettoie et filtre la colonne Price"""
    df["Price"] = pd.to_numeric(df["Price"], errors="coerce")
    df = df[df["Price"].notna() & (df["Price"] > 0)]
    print(f"‚úÖ Price nettoy√©: {len(df)} lignes avec prix valide")
    return df

def remove_high_missing_columns(df, threshold=40):
    """Supprime colonnes avec >threshold% de manquants"""
    missing_pct = (df.isnull().sum() / len(df) * 100)
    cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()
    
    if 'Year of production' in cols_to_drop:
        cols_to_drop.remove('Year of production')
        print(f"‚ö†Ô∏è 'Year of production' conserv√©e malgr√© {missing_pct['Year of production']:.1f}% manquants")
    
    df = df.drop(columns=cols_to_drop)
    print(f"‚úÖ Colonnes supprim√©es (>40%): {cols_to_drop}")
    return df

# ============================================================================
# √âTAPE 2: IMPUTATION AVEC VALIDATION CROIS√âE
# ============================================================================

def validate_imputation(df, column, impute_func, n_splits=5):
    """
    Valide la qualit√© d'une imputation par validation crois√©e
    """
    print(f"\nüîç Validation crois√©e de l'imputation pour '{column}'")
    
    # S√©parer les lignes avec et sans valeur
    known = df[df[column].notna()].copy()
    missing = df[df[column].isna()].copy()
    
    if len(known) < n_splits * 2:
        print(f"   ‚ö†Ô∏è  Trop peu de donn√©es connues pour la validation crois√©e")
        return None
    
    # Cr√©er des folds
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)
    
    errors = []
    for fold, (train_idx, val_idx) in enumerate(kf.split(known), 1):
        # Simuler des valeurs manquantes
        train_data = known.iloc[train_idx].copy()
        val_data = known.iloc[val_idx].copy()
        
        # Valeurs r√©elles √† pr√©dire
        true_values = val_data[column].copy()
        
        # Masquer les valeurs de validation
        val_data_copy = val_data.copy()
        val_data_copy[column] = np.nan
        
        # Combiner train et val pour l'imputation
        temp_df = pd.concat([train_data, val_data_copy])
        
        # Appliquer l'imputation
        temp_df = impute_func(temp_df)
        
        # R√©cup√©rer les valeurs imput√©es
        imputed_values = temp_df.loc[val_data.index, column]
        
        # Calculer l'erreur
        mae = np.mean(np.abs(imputed_values - true_values))
        errors.append(mae)
        print(f"   Fold {fold}: MAE = {mae:.2f}")
    
    mean_error = np.mean(errors)
    std_error = np.std(errors)
    print(f"   üìä MAE moyenne: {mean_error:.2f} ¬± {std_error:.2f}")
    
    return {
        'mean_mae': mean_error,
        'std_mae': std_error,
        'fold_errors': errors
    }

def impute_year_of_production(df, validate=True):
    """Impute Year of production par m√©diane de Brand avec validation"""
    
    def impute_func(temp_df):
        brand_median = temp_df.groupby('Brand')['Year of production'].median()
        
        def impute_row(row):
            if pd.isna(row['Year of production']):
                brand = row['Brand']
                if pd.notna(brand) and brand in brand_median.index:
                    return brand_median[brand]
                return 2020
            return row['Year of production']
        
        temp_df['Year of production'] = temp_df.apply(impute_row, axis=1)
        return temp_df
    
    # Validation crois√©e
    if validate and 'Year of production' in df.columns:
        validation_result = validate_imputation(df, 'Year of production', impute_func)
    
    # Application r√©elle
    df = impute_func(df)
    print(f"‚úÖ Year of production imput√©e (m√©diane par Brand)")
    if validate and validation_result:
        print(f"   Qualit√© estim√©e: MAE = {validation_result['mean_mae']:.2f}")
    return df

def impute_categorical(df, validate=True):
    """Impute les colonnes cat√©gorielles par mode avec validation"""
    
    cat_cols = df.select_dtypes(include=['object']).columns
    
    for col in cat_cols:
        if df[col].isnull().sum() > 0:
            mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'
            
            # Validation simple (pas de CV pour cat√©goriel, juste fr√©quence)
            missing_before = df[col].isnull().sum()
            df[col] = df[col].fillna(mode_val)
            print(f"‚úÖ {col} imput√©e par mode: '{mode_val}' ({missing_before} valeurs)")
    
    return df

def impute_numerical(df, validate=True):
    """Impute les num√©riques (hors Price, Year) par m√©diane avec validation"""
    
    num_cols = df.select_dtypes(include=[np.number]).columns
    exclude = ['Price', 'Year of production']
    
    for col in num_cols:
        if col not in exclude and df[col].isnull().sum() > 0:
            
            def impute_func(temp_df):
                median_val = temp_df[col].median()
                temp_df[col] = temp_df[col].fillna(median_val)
                return temp_df
            
            # Validation crois√©e
            if validate:
                validation_result = validate_imputation(df, col, impute_func)
            
            # Application
            median_val = df[col].median()
            missing_before = df[col].isnull().sum()
            df[col] = df[col].fillna(median_val)
            print(f"‚úÖ {col} imput√©e par m√©diane: {median_val:.2f} ({missing_before} valeurs)")
            if validate and validation_result:
                print(f"   Qualit√© estim√©e: MAE = {validation_result['mean_mae']:.2f}")
    
    return df

# ============================================================================
# √âTAPE 3: GESTION DES ANOMALIES
# ============================================================================

def fix_year_anomalies(df):
    """Corrige les ann√©es aberrantes"""
    mask = (df['Year of production'] < 1900) | (df['Year of production'] > CURRENT_YEAR)
    anomalies = mask.sum()
    
    brand_median = df.groupby('Brand')['Year of production'].median()
    
    df.loc[mask, 'Year of production'] = df.loc[mask, 'Brand'].map(
        lambda x: brand_median.get(x, 2020)
    )
    print(f"‚úÖ Ann√©es aberrantes corrig√©es: {anomalies}")
    return df

def winsorize_face_area(df):
    """Winsorise Face Area au 99√®me percentile"""
    if 'Face Area' in df.columns:
        p99 = df['Face Area'].quantile(0.99)
        df['Face Area'] = df['Face Area'].clip(upper=p99)
        print(f"‚úÖ Face Area winsoris√© (p99={p99:.2f})")
    return df

# ============================================================================
# √âTAPE 4: FEATURE ENGINEERING
# ============================================================================

def create_features(df):
    """Cr√©e toutes les nouvelles features"""
    print(f"\n{'='*50}")
    print("FEATURE ENGINEERING")
    print(f"{'='*50}")
    
    # Age
    df['age'] = CURRENT_YEAR - df['Year of production']
    print(f"‚úÖ 'age' cr√©√©e (moy: {df['age'].mean():.1f} ans)")
    
    # Is modern
    df['is_modern'] = (df['Year of production'] >= 2000).astype(int)
    print(f"‚úÖ 'is_modern' cr√©√©e ({df['is_modern'].sum()} montres)")
    
    # Log price (cible pour la r√©gression)
    df['price_log'] = np.log1p(df['Price'])
    print(f"‚úÖ 'price_log' cr√©√©e")
    
    # Seller reputation score
    seller_cols = ['Fast Shipper', 'Trusted Seller', 'Punctuality']
    available = [c for c in seller_cols if c in df.columns]
    if available:
        df['seller_reputation_score'] = df[available].mean(axis=1)
        print(f"‚úÖ 'seller_reputation_score' cr√©√©e")
    
    # Scope score
    scope_map = {
        'Original box, original papers': 3,
        'Original box, no original papers': 2,
        'Original papers, no original box': 1,
        'No original box, no original papers': 0
    }
    if 'Scope of delivery' in df.columns:
        df['scope_score'] = df['Scope of delivery'].map(scope_map).fillna(0)
        print(f"‚úÖ 'scope_score' cr√©√©e")
    
    # Flags d'anomalies
    df['price_anomaly_low'] = (df['Price'] < 50).astype(int)
    df['price_anomaly_high'] = (df['Price'] > 1000000).astype(int)
    print(f"‚úÖ Flags d'anomalies cr√©√©s")
    
    # Consistance vendeur
    if all(c in df.columns for c in ['Watches Sold by the Seller', 'Active listing of the seller']):
        df['seller_consistent'] = (
            df['Watches Sold by the Seller'] >= df['Active listing of the seller']
        ).astype(int)
        print(f"‚úÖ 'seller_consistent' cr√©√©e")
    
    return df

# ============================================================================
# PIPELINE COMPLET
# ============================================================================

def run_preprocessing(validate_imputations=True):
    """Ex√©cute tout le preprocessing"""
    print(f"\n{'='*70}")
    print("PREPROCESSING ET FEATURE ENGINEERING")
    print(f"{'='*70}")
    print(f"Validation des imputations: {'Activ√©e' if validate_imputations else 'D√©sactiv√©e'}")
    
    # Chargement
    df = load_raw()
    print(f"\nüì• Entr√©e: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    
    # Nettoyage
    print(f"\n{'='*50}")
    print("NETTOYAGE")
    print(f"{'='*50}")
    df = remove_duplicates(df)
    df = clean_price(df)
    df = remove_high_missing_columns(df)
    
    # Imputation avec validation
    print(f"\n{'='*50}")
    print("IMPUTATION (avec validation crois√©e)")
    print(f"{'='*50}")
    df = impute_year_of_production(df, validate=validate_imputations)
    df = impute_categorical(df, validate=validate_imputations)
    df = impute_numerical(df, validate=validate_imputations)
    
    # Anomalies
    print(f"\n{'='*50}")
    print("CORRECTION ANOMALIES")
    print(f"{'='*50}")
    df = fix_year_anomalies(df)
    df = winsorize_face_area(df)
    
    # Feature Engineering
    df = create_features(df)
    
    # V√©rification finale
    print(f"\n{'='*50}")
    print("V√âRIFICATION FINALE")
    print(f"{'='*50}")
    print(f"Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
    print(f"Valeurs manquantes: {df.isnull().sum().sum()}")
    print(f"Colonnes: {list(df.columns)}")
    
    # Sauvegarde
    df.to_csv(CLEANED_CSV, index=False, sep=';')
    print(f"\nüíæ Sauvegard√©: {CLEANED_CSV}")
    
    return df

if __name__ == "__main__":
    from pathlib import Path
    # Par d√©faut, validation activ√©e
    df_cleaned = run_preprocessing(validate_imputations=True)